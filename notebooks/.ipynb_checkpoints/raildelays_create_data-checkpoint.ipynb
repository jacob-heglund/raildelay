{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:37:57.422246Z",
     "start_time": "2020-02-28T20:37:56.984534Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import collections\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "\n",
    "import pdb\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "os.chdir(\"/home/jacobheglund/dev/raildelay\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:38:20.698318Z",
     "start_time": "2020-02-28T20:38:20.688354Z"
    }
   },
   "outputs": [],
   "source": [
    "# include all these data points\n",
    "# Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "# Path(ckpt_dir).mkdir(parents=True, exist_ok=True)\n",
    "dataset_name = \"raildelays\"\n",
    "raw_dir = \"./data/raw/\" + dataset_name + \"/\"\n",
    "interim_dir = \"./data/interim/\" + dataset_name + \"/\"\n",
    "processed_dir = \"./data/processed/\" + dataset_name + \"/\"\n",
    "\n",
    "Path(raw_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(interim_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(processed_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#TODO have a standard way of naming these things for the model (#code_goals)\n",
    "## ex: adj_path = processed_dir + \"adj.npy\"\n",
    "## ex: dataset_path = processed_dir + \"dataset.npy\"\n",
    "\n",
    "raw_data_path_2016 = raw_dir + \"HSP_2016_DID_PAD.txt\"\n",
    "raw_data_path_2017 = raw_dir + \"HSP_2017_DID_PAD.txt\"\n",
    "stop_position_path = raw_dir + \"stations.csv\"\n",
    "adjlist_path = interim_dir + \"adjlist.txt\"\n",
    "adj_path = processed_dir + \"adj.npy\"\n",
    "dataset_path = processed_dir + \"dataset.npy\"\n",
    "\n",
    "# plotting options\n",
    "show_plots = False\n",
    "figsize = (16, 9)\n",
    "dpi = 200\n",
    "node_size = 50\n",
    "# xlim, ylim based on min/max lat/long for the included stations in the dataset\n",
    "xlim = (-4.5, 0.5)\n",
    "ylim = (50.75, 52.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link-Based Node Formulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:38:29.330870Z",
     "start_time": "2020-02-28T20:38:21.961100Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(raw_data_path_2016, sep=\",\")\n",
    "df2 = pd.read_csv(raw_data_path_2017, sep=\",\")\n",
    "df2 = df2.drop([\"Unnamed: 0\"], axis=1)\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "# 2017 includes XXXPAD and PADXXX trips, 2016 does not, so remove the outbound trips\n",
    "df = df[df[\"station_origin\"] != \"PAD\"]\n",
    "\n",
    "# remove any stops that are included in either year but not the other\n",
    "stops_2016 = np.unique(df1[\"station_curr\"])\n",
    "stops_2017 = np.unique(df2[\"station_curr\"])\n",
    "stops_drop = []\n",
    "\n",
    "for i in stops_2017:\n",
    "    if i not in stops_2016:\n",
    "        stops_drop.append(i)\n",
    "for i in stops_2016:\n",
    "    if i not in stops_2017:\n",
    "        stops_drop.append(i)\n",
    "for i in stops_drop:\n",
    "    df = df[df[\"station_curr\"] != i]\n",
    "    \n",
    "\n",
    "# remove stations that serve an average of less than stop_thres trains per day \n",
    "stations = np.unique(df[\"station_curr\"])\n",
    "n_days = len(np.unique(df[\"date\"]))\n",
    "# stops per station\n",
    "sps = dict(collections.Counter(df[\"station_curr\"]))\n",
    "sps = dict(sorted(sps.items()))\n",
    "\n",
    "# if you change this value you will have to manually change the corridors and available routes since\n",
    "# that process is difficult to automate at this point in time\n",
    "stop_thres = 1\n",
    "counter = 0\n",
    "for i in list(sps):\n",
    "    if (sps[i] / n_days) < stop_thres:\n",
    "        # find set of RID's associated with these\n",
    "        df_tmp = df.loc[df[\"station_curr\"] == i]\n",
    "        RID_drop = np.unique(df_tmp[\"RID\"])\n",
    "        # remove train routes that stop at any of the non-included stations\n",
    "        for j in RID_drop:\n",
    "            df = df.loc[df[\"RID\"] != j]\n",
    "        sps.pop(i)\n",
    "\n",
    "df = df.reset_index()\n",
    "df = df.drop([\"index\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:38:29.401975Z",
     "start_time": "2020-02-28T20:38:29.332205Z"
    }
   },
   "outputs": [],
   "source": [
    "stops_data_period = np.unique(df[\"station_curr\"])\n",
    "df_sp = pd.read_csv(stop_position_path)\n",
    "stop_position = {}\n",
    "for i in range(len(df_sp)):\n",
    "    curr_row = df_sp[i:i+1]\n",
    "    if curr_row[\"crs\"].item() in stops_data_period:\n",
    "        stop_position[curr_row[\"crs\"].item()] = (curr_row[\"lat\"].item(), curr_row[\"lon\"].item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:38:29.445111Z",
     "start_time": "2020-02-28T20:38:29.403411Z"
    }
   },
   "outputs": [],
   "source": [
    "# all corridors are defined in linear order\n",
    "## once a train gets on a corridor, it travels through all the stations on that corridor\n",
    "## although it does not necessarily stop at any of the stations on that corridor\n",
    "\n",
    "# the corridors are designed with the threshold of at least 1 train per day on average\n",
    "## it keeps out the \"end of line\" stations which just add noise to the data\n",
    "## in total this removes ~1% of the data\n",
    "\n",
    "# list of railway stations\n",
    "## https://en.wikipedia.org/wiki/UK_railway_stations_%E2%80%93_A\n",
    "\n",
    "# station map for defining corridors\n",
    "## https://www.gwr.com/plan-journey/stations-and-routes \n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# Northwest Corridor\n",
    "# --------------------------------------------\n",
    "# SWA - SWANSEA - START OF CORRIDOR\n",
    "# NTH - NEATH\n",
    "# PTA - PORT TALBOT PARKWAY\n",
    "# BGN - BRIDGEND\n",
    "# CDF - CARDIFF CENTRAL\n",
    "# NWP - NEWPORT\n",
    "# BPW - BRISTOL PARKWAY\n",
    "# SWI - SWINDON - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# Southwest Corridor\n",
    "# --------------------------------------------\n",
    "# WSM - WESTON-SUPER-MARE - START OF CORRIDOR\n",
    "# WNM - WESTON MILTON\n",
    "# WOR - WORLE\n",
    "# YAT - YATTON\n",
    "# NLS - NAILSEA AND BLACKWELL\n",
    "# BRI - BRISTON TEMPLE MEADS\n",
    "# BTH - BATH SPA\n",
    "# CPM - CHIPPENHAM\n",
    "# SWI - SWINDON - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# Middle Connecting Corridor\n",
    "# --------------------------------------------\n",
    "# SWI - SWINDON - START OF CORRIDOR\n",
    "# DID - DIDCOT PARKWAY - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# North Central Corridor\n",
    "# --------------------------------------------\n",
    "# BAN - BANBURY - START OF CORRIDOR\n",
    "# KGS - KINGS SUTTON\n",
    "# HYD - HEYFORD\n",
    "# TAC - TACKLEY\n",
    "# OXF - OXFORD\n",
    "# RAD - RADLEY\n",
    "# CUM - CULHAM\n",
    "# APF - APPLEFORD\n",
    "# DID - DIDCOT PARKWAY - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# Eastern Corridor\n",
    "# --------------------------------------------\n",
    "# DID - DIDCOT PARKWAY - START OF CORRIDOR\n",
    "# CHO - CHOLSEY\n",
    "# GOR - GORING AND STREATLEY\n",
    "# PAN - PANGBOURNE\n",
    "# TLH - TILEHURST\n",
    "# RDG - READING\n",
    "# TWY - TWYFORD\n",
    "# MAI - MAIDENHEAD\n",
    "# BNM - BURNHAM\n",
    "# SLO - SLOUGH\n",
    "# LNY - LANGLEY\n",
    "# IVR - IVER\n",
    "# WDT - WEST DRAYTON\n",
    "# HAY - HAYES AND HARLINGTON\n",
    "# STL - SOUTHALL\n",
    "# EAL - EALING BROADWAY\n",
    "# PAD - LONDON PADDINGTON - END OF CORRIDOR\n",
    "\n",
    "# used in defining the graph\n",
    "corridor_list = [\n",
    "    [\"SWA\", \"NTH\", \"PTA\", \"BGN\", \"CDF\", \"NWP\", \"BPW\", \"SWI\"],\n",
    "    [\"WSM\", \"WNM\", \"WOR\", \"YAT\", \"NLS\", \"BRI\", \"BTH\", \"CPM\", \"SWI\"],\n",
    "    [\"SWI\", \"DID\"],\n",
    "    [\"BAN\", \"KGS\", \"HYD\", \"TAC\", \"OXF\", \"RAD\", \"CUM\", \"APF\", \"DID\"],\n",
    "    [\"DID\", \"CHO\", \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\", \"BNM\", \"SLO\", \"LNY\",\n",
    "     \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"]\n",
    "]\n",
    "\n",
    "# rearrange sps with stops grouped by corridor\n",
    "sps_tmp = {}\n",
    "for corridor in corridor_list:\n",
    "    for j in corridor:\n",
    "        keys = list(sps_tmp)\n",
    "        if j not in keys:\n",
    "            sps_tmp[j] = sps[j]\n",
    "sps = sps_tmp\n",
    "\n",
    "# used in finding links between stops\n",
    "available_routes = [\n",
    "    [\"SWA\", \"NTH\", \"PTA\", \"BGN\", \"CDF\", \"NWP\", \"BPW\", \"SWI\", \"DID\", \"CHO\",\n",
    "     \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\", \"BNM\", \"SLO\", \"LNY\",\n",
    "     \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"],\n",
    "    \n",
    "    [\"WSM\", \"WNM\", \"WOR\", \"YAT\", \"NLS\", \"BRI\", \"BTH\", \n",
    "     \"CPM\", \"SWI\", \"DID\", \"CHO\", \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\",\n",
    "     \"BNM\", \"SLO\", \"LNY\", \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"],\n",
    "    \n",
    "    [\"BAN\", \"KGS\", \"HYD\", \"TAC\", \"OXF\", \"RAD\", \"CUM\", \"APF\", \"DID\", \"CHO\",\n",
    "     \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\", \"BNM\", \"SLO\", \"LNY\",\n",
    "     \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:38:29.450544Z",
     "start_time": "2020-02-28T20:38:29.446522Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_links(stop_1, stop_2, available_routes):\n",
    "    # returns a list of all links between two stops on a route\n",
    "    # this relies on having an array of possible routes\n",
    "    #EX: my_list_1 = get_links(\"WDT\", \"PAD\")\n",
    "    # my_list_1 = [\"WDTHHAY\", \"HAYSTL\", \"STLEAL\", \"EALPAD\"]\n",
    "    done = 0\n",
    "    link_list = []\n",
    "    for i in range(len(available_routes)):\n",
    "        curr_route = available_routes[i]\n",
    "        if (stop_1 in curr_route) and (stop_2 in curr_route) and (not done):\n",
    "            done = 1\n",
    "            stop_1_idx = curr_route.index(stop_1)\n",
    "            stop_2_idx = curr_route.index(stop_2)            \n",
    "            for j in range(stop_1_idx, stop_2_idx):                \n",
    "                link_list.append(curr_route[j] + curr_route[j+1])\n",
    "        \n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:38:29.710514Z",
     "start_time": "2020-02-28T20:38:29.451530Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataframe setup\n",
    "\n",
    "# we're using scheduled times, but there is no reason that we couldn't use actual arrival/departure times\n",
    "## the reason we don't is b/c we would have scheduled tiems available in the real world prediction\n",
    "## maybe a better way would be to use scheduled times in the future predictions,\n",
    "## and actual times in for input data but this just adds needless complecation for a first stab at the problem\n",
    "\n",
    "# OD does not index the links, just the origin and destination of the route\n",
    "df[\"OD\"] = df[\"station_origin\"] + df[\"station_destination\"]\n",
    "\n",
    "# get integer time values for all rows (arrival and departure)\n",
    "df[\"dep_sched_int\"] = df[\"departure_sched\"]\n",
    "df[\"dep_sched_int\"][df[\"dep_sched_int\"] == \"terminating\"] = df[\"arrival_sched\"]\n",
    "df[\"dep_actual_int\"] = df[\"departure_actual\"]\n",
    "df[\"dep_actual_int\"][df[\"dep_actual_int\"] == \"terminating\"] = df[\"arrival_actual\"]\n",
    "\n",
    "# this one is different b/c \"starting\" arrival delay must be 0\n",
    "df[\"arr_sched_int\"] = df[\"arrival_sched\"]\n",
    "df[\"arr_sched_int\"][df[\"arr_sched_int\"] == \"starting\"] = df[\"departure_actual\"]\n",
    "df[\"arr_actual_int\"] = df[\"arrival_actual\"]\n",
    "df[\"arr_actual_int\"][df[\"arr_actual_int\"] == \"starting\"] = df[\"departure_actual\"]\n",
    "\n",
    "# get scheduled arrival and departure as datetime\n",
    "df[\"arr_sched_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"arr_sched_int\"].astype(str))\n",
    "df[\"dep_sched_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"dep_sched_int\"].astype(str))\n",
    "df[\"arr_actual_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"arr_actual_int\"].astype(str))\n",
    "df[\"dep_actual_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"dep_actual_int\"].astype(str)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:30.832313Z",
     "start_time": "2020-02-28T20:38:29.711520Z"
    }
   },
   "outputs": [],
   "source": [
    "# find the delay that was generated by the the previous section of track for each stop\n",
    "## arrival delay is positive if the train was late, and negative if the train was early\n",
    "\n",
    "# also get the departure time of the previous station (we need this to attribute the delay to the proper\n",
    "# time periods)\n",
    "\n",
    "# get the set of links each train travelled over between consecutive stops\n",
    "df[\"cum_arr_delay\"] = ((df[\"arr_actual_datetime\"] - df[\"arr_sched_datetime\"]).dt.total_seconds()) / 60\n",
    "init_list = []\n",
    "for i in range(len(df)):\n",
    "    init_list.append([])\n",
    "df[\"prev_links\"] = init_list\n",
    "\n",
    "\n",
    "# make sure we're only combining data from one train trip at a time\n",
    "for row_count in range(len(df)):\n",
    "    curr_row = df[row_count:row_count+1]\n",
    "    if (curr_row[\"arrival_sched\"].item() == \"starting\"):\n",
    "        df.at[row_count, \"prev_link_avg_datetime\"] = df.at[row_count, \"dep_sched_datetime\"]\n",
    "        df.at[row_count, \"prev_link_delay\"] = 0.\n",
    "        df.at[row_count, \"prev_links\"] = []\n",
    "        df.at[row_count, \"prev_station_dep_sched_datetime\"] = df.at[row_count, \"dep_sched_datetime\"]\n",
    "\n",
    "    else:\n",
    "        prev_row = df[row_count-1:row_count]\n",
    "\n",
    "        t0 = prev_row[\"dep_sched_datetime\"].item()\n",
    "        t1 = curr_row[\"arr_sched_datetime\"].item()\n",
    "\n",
    "        # average time at which the train was running on previous link\n",
    "        df.at[row_count, \"prev_link_avg_datetime\"] = t0 + ((t1-t0) / 2)\n",
    "        prev_links = get_links(prev_row[\"station_curr\"].item(), curr_row[\"station_curr\"].item(), available_routes)\n",
    "        df.at[row_count, \"prev_links\"] = prev_links\n",
    "        \n",
    "        # departure time from previous station\n",
    "        df.at[row_count, \"prev_station_dep_sched_datetime\"] = prev_row[\"dep_sched_datetime\"].item()\n",
    "\n",
    "        # average delay per link\n",
    "        # average the delay over # of links passed through by trains\n",
    "        ## Train 2: A -> D (doesn't stop at B or C but passes through)\n",
    "        ## 6 minute arrival delay at D\n",
    "        ## average delay of 6 minutes / 3 links = 2 minutes/link is attributed to each link b/c the data isn't\n",
    "        ## granular enough to provide a more accurate estimation\n",
    "        n_links = len(prev_links)\n",
    "        if (n_links == 0):\n",
    "            n_links = 1\n",
    "            \n",
    "        df.at[row_count, \"prev_link_delay\"] = (curr_row[\"cum_arr_delay\"].item() - prev_row[\"cum_arr_delay\"].item()) \\\n",
    "        / n_links\n",
    "        \n",
    "    row_count += 1\n",
    "        \n",
    "# alphabetize columns\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation and Visualization of Rail Network Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:30.836297Z",
     "start_time": "2020-02-28T20:39:30.833551Z"
    }
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "stop_idx = {}\n",
    "idx_stop = {}\n",
    "for i in list(sps):\n",
    "    stop_idx[i] = counter\n",
    "    idx_stop[counter] = str(i)\n",
    "    counter += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:30.842580Z",
     "start_time": "2020-02-28T20:39:30.838529Z"
    }
   },
   "outputs": [],
   "source": [
    "# # processing to verify the corridors I defined\n",
    "# # Southwest Corridor\n",
    "# # check if any trains starting in TAU go through BPW or if they all go through SWI\n",
    "# ## Result: They all go through SWI, so the SW Corridor goes from TAU to SWI without branching off in this dataset\n",
    "# ## Result: The Northwest and Southwest Corridors only meet at SWI for this dataset\n",
    "\n",
    "# # North Central Corridor\n",
    "# ## all trains going from BAN to PAD must pass through DID to be part of this dataset\n",
    "# ## therefore there are no trains going from APF to CHO\n",
    "# ## no trains go from BAN and pass through SWI\n",
    "\n",
    "# df1 = df.copy()\n",
    "# df1 = df1.loc[df1[\"station_origin\"] == \"BAN\"]\n",
    "# df1 = df1.loc[df1[\"station_curr\"] == \"SWI\"]\n",
    "\n",
    "\n",
    "# df1.head()\n",
    "# print(len(df1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:30.854739Z",
     "start_time": "2020-02-28T20:39:30.844673Z"
    }
   },
   "outputs": [],
   "source": [
    "# create graph representing rail network\n",
    "directed_graph = False\n",
    "\n",
    "total_str = \"\"\n",
    "for corridor in corridor_list:\n",
    "    if directed_graph:\n",
    "        # add option for directed adj\n",
    "        # how do other traffic papers accomplish this with their model?\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        for j in range(len(corridor)):\n",
    "            tmp_str = \"\"\n",
    "            if (j == 0):\n",
    "                tmp_str += str(stop_idx[corridor[j]])\n",
    "                tmp_str += \" \"\n",
    "                tmp_str += str(stop_idx[corridor[j+1]])\n",
    "\n",
    "            elif (j == len(corridor)-1):\n",
    "                tmp_str += str(stop_idx[corridor[j]])\n",
    "                tmp_str += \" \"\n",
    "                tmp_str += str(stop_idx[corridor[j-1]])\n",
    "\n",
    "            else:\n",
    "                tmp_str += str(stop_idx[corridor[j]])\n",
    "                tmp_str += \" \"\n",
    "                tmp_str += str(stop_idx[corridor[j-1]])\n",
    "                tmp_str += \" \"\n",
    "                tmp_str += str(stop_idx[corridor[j+1]])\n",
    "\n",
    "            total_str += tmp_str\n",
    "            total_str += \"\\n\"\n",
    "        \n",
    "with open(adjlist_path, \"w\") as f:\n",
    "    f.write(total_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:30.859450Z",
     "start_time": "2020-02-28T20:39:30.856110Z"
    }
   },
   "outputs": [],
   "source": [
    "idx_position = {}\n",
    "counter = 0\n",
    "for i in stop_position:\n",
    "    idx_position[stop_idx[i]] = (stop_position[i][1],stop_position[i][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:30.865233Z",
     "start_time": "2020-02-28T20:39:30.860854Z"
    }
   },
   "outputs": [],
   "source": [
    "G = nx.read_adjlist(adjlist_path, nodetype=int)\n",
    "if show_plots:\n",
    "    fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    nx.draw(G, idx_position, node_size=node_size)\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:30.870482Z",
     "start_time": "2020-02-28T20:39:30.866529Z"
    }
   },
   "outputs": [],
   "source": [
    "if show_plots:\n",
    "    fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    nx.draw_networkx_labels(G, idx_position, idx_stop, font_size=10)\n",
    "    nx.draw_networkx_edges(G, idx_position)\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:30.875336Z",
     "start_time": "2020-02-28T20:39:30.871920Z"
    }
   },
   "outputs": [],
   "source": [
    "LG = nx.line_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:30.881576Z",
     "start_time": "2020-02-28T20:39:30.876666Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the position of the edges as average of station positions\n",
    "edge_position = {}\n",
    "for edge in LG.nodes():\n",
    "    station_1 = int(edge[0])\n",
    "    station_2 = int(edge[1])\n",
    "    x1, x2 = idx_position[station_1][0], idx_position[station_2][0]\n",
    "    y1, y2 = idx_position[station_1][1], idx_position[station_2][1]\n",
    "    \n",
    "    x_edge = (x1+x2)/2\n",
    "    y_edge = (y1+y2)/2\n",
    "    edge_position[edge] = (x_edge, y_edge) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:30.886477Z",
     "start_time": "2020-02-28T20:39:30.882575Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get edge labels\n",
    "edge_label = {}\n",
    "for edge in LG.nodes():\n",
    "    station_1 = int(edge[0])\n",
    "    station_2 = int(edge[1])\n",
    "    \n",
    "    # TODO i'm not sure why these two cases mess it up, but\n",
    "    # SWICPM should be CPMSWI\n",
    "    # DIDAPF should be APFDID\n",
    "    # something about the fact that they have multiple connections or are part of a complex junction\n",
    "\n",
    "    if (idx_stop[station_1] == \"SWI\") and (idx_stop[station_2] == \"CPM\") or \\\n",
    "        (idx_stop[station_1] == \"DID\") and (idx_stop[station_2] == \"APF\"):\n",
    "        station_tmp = station_1\n",
    "        station_1 = station_2\n",
    "        station_2 = station_tmp\n",
    "    \n",
    "    edge_label[edge] = idx_stop[station_1] + idx_stop[station_2]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:30.890695Z",
     "start_time": "2020-02-28T20:39:30.887519Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if show_plots:\n",
    "    fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    nx.draw(LG, edge_position, node_size=node_size)\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:30.896033Z",
     "start_time": "2020-02-28T20:39:30.892041Z"
    }
   },
   "outputs": [],
   "source": [
    "if show_plots:\n",
    "    fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    nx.draw_networkx_labels(LG, edge_position, edge_label, font_size=7)\n",
    "    nx.draw_networkx_edges(LG, edge_position)\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:30.903986Z",
     "start_time": "2020-02-28T20:39:30.897829Z"
    }
   },
   "outputs": [],
   "source": [
    "# add self loops to graph\n",
    "for i in LG.nodes:\n",
    "    LG.add_edge(i, i)\n",
    "\n",
    "adj = nx.adjacency_matrix(LG).todense()\n",
    "adj = np.array(adj).astype(\"d\")\n",
    "np.save(adj_path, adj)\n",
    "if show_plots:\n",
    "    plt.matshow(adj)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Rail Link Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:30.908523Z",
     "start_time": "2020-02-28T20:39:30.905265Z"
    }
   },
   "outputs": [],
   "source": [
    "# # this shows that 99% of trains have a travel time of less than 30 minutes, so we should have a smaller time window\n",
    "# # let's try 10 minutes\n",
    "# a = max(df[\"travel_time_sched\"])\n",
    "# b = df.loc[df[\"travel_time_sched\"] > 30]\n",
    "# print(len(b))\n",
    "# print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:31.080884Z",
     "start_time": "2020-02-28T20:39:30.909703Z"
    }
   },
   "outputs": [],
   "source": [
    "## find starting and ending time for the day\n",
    "dt = 10 # minutes\n",
    "\n",
    "def round_time(t_input, dt, direction=\"floor\"):\n",
    "    # \n",
    "    if direction == \"floor\":\n",
    "        n_minutes = ((pd.to_timedelta(str(t_input[0:2] + \":\" + t_input[2:4] + \":00\")).seconds / 60) // dt * dt)\n",
    "        return pd.to_timedelta(n_minutes, unit=\"m\")\n",
    "    \n",
    "    elif direction == \"ceil\":\n",
    "        n_minutes = ((pd.to_timedelta(str(t_input[0:2] + \":\" + t_input[2:4] + \":00\")).seconds / 60 + dt) // dt * dt)\n",
    "        return pd.to_timedelta(n_minutes, unit=\"m\")\n",
    "    \n",
    "t_min = min(df[\"dep_sched_int\"])\n",
    "t_max = max(df[\"dep_sched_int\"])\n",
    "t_start = round_time(t_min, dt)\n",
    "t_end = round_time(t_max, dt, \"ceil\")\n",
    "\n",
    "# unique times during each day\n",
    "time_list = []\n",
    "t_curr = t_start\n",
    "\n",
    "while t_curr < t_end:\n",
    "    time_list.append(t_curr)\n",
    "    t_curr += pd.Timedelta(str(dt) + \"minutes\")\n",
    "\n",
    "# unique dates of the year\n",
    "date_list = np.unique(df[\"date\"])\n",
    "\n",
    "datetime_list = []\n",
    "for i in date_list:\n",
    "    t0 = pd.to_datetime(i + \" 00:00:00\")\n",
    "        \n",
    "    for j in time_list:\n",
    "        t_curr = t0 + j\n",
    "        datetime_list.append(t_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:39:31.094818Z",
     "start_time": "2020-02-28T20:39:31.082053Z"
    }
   },
   "outputs": [],
   "source": [
    "## construct useful mappings\n",
    "# mapping from node description to unique index\n",
    "LG_node_idx = {}\n",
    "# mapping from unique index to node description \n",
    "LG_idx_node = {}\n",
    "counter = 0\n",
    "for i in LG.nodes():\n",
    "    LG_node_idx[i] = counter\n",
    "    LG_idx_node[counter] = i\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "# mapping from node description to label (i.e. \"SWAPAD\")\n",
    "LG_node_label = edge_label\n",
    "# mapping from label to node description\n",
    "LG_label_node = {}\n",
    "for i in LG_node_label:\n",
    "    LG_label_node[LG_node_label[i]] = i\n",
    "\n",
    "\n",
    "# mapping from time of year to time data index\n",
    "datetime_idx = {}\n",
    "# mapping from time data index to time of year\n",
    "idx_datetime = {}\n",
    "counter = 0\n",
    "for i in datetime_list:\n",
    "    datetime_idx[i] = counter\n",
    "    idx_datetime[counter] = i\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:41:02.188681Z",
     "start_time": "2020-02-28T20:39:31.096058Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_nodes = len(adj)\n",
    "n_timesteps = len(datetime_list)\n",
    "n_features = 1 #  delay per link\n",
    "n_timesteps_per_day = ((t_end - t_start)/dt).seconds / 60\n",
    "delay_dataset = np.zeros((n_timesteps, n_nodes, n_features))\n",
    "n_trains_total = np.zeros((n_nodes, 1))\n",
    "\n",
    "for i in range(len(datetime_list)-1):\n",
    "    # get set of trains running during current time period    \n",
    "    t0, t1 = datetime_list[i], datetime_list[i+1]\n",
    "    \n",
    "    # attribute the link delay to all relevant times \n",
    "    ## i.e. from scheduled departure of previous station to scheduled arrival at current station\n",
    "    t_avg_cond = ((t0 <= df[\"prev_link_avg_datetime\"]) & (df[\"prev_link_avg_datetime\"] <= t1))\n",
    "    t_arr_cond = ((t0 <= df[\"arr_sched_datetime\"]) & (df[\"arr_sched_datetime\"] <= t1))\n",
    "    t_dep_cond = ((t0 <= df[\"prev_station_dep_sched_datetime\"]) & (df[\"prev_station_dep_sched_datetime\"] <= t1))\n",
    "    \n",
    "    df_tmp = df.loc[t_avg_cond | t_dep_cond | t_arr_cond]\n",
    "    n_trains_link = np.zeros((n_nodes, 1))\n",
    "\n",
    "    for j in range(len(df_tmp)):\n",
    "        curr_row = df_tmp[j:j+1]\n",
    "        prev_links = curr_row[\"prev_links\"].item()\n",
    "\n",
    "        for k in range(len(prev_links)):\n",
    "            link = prev_links[k]\n",
    "            link_idx = LG_node_idx[LG_label_node[link]]\n",
    "\n",
    "            # sum the average delay experienced by trains passing through link during the time period\n",
    "            delay_dataset[i, link_idx, 0] += curr_row[\"prev_link_delay\"].item()\n",
    "            \n",
    "            # enumerate trains passing through each link during the time period\n",
    "            n_trains_link[link_idx] += 1\n",
    "            \n",
    "    n_trains_total += n_trains_link\n",
    "    \n",
    "    # calculate delay over each link averaged by number of trains that passed through the link during the time period\n",
    "    divisor = n_trains_link + np.where(n_trains_link == 0, 1, 0)\n",
    "    delay_dataset[i, :, :] = delay_dataset[i, :, :] / divisor\n",
    "\n",
    "np.save(dataset_path, delay_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:41:02.193692Z",
     "start_time": "2020-02-28T20:41:02.190644Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"n_timesteps_per_day:\", n_timesteps_per_day)\n",
    "print(\"n_nodes\", n_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
