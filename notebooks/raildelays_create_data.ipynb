{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:59:43.907846Z",
     "start_time": "2020-02-27T21:59:43.901916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jacobheglund/dev/raildelays\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import collections\n",
    "import networkx as nx\n",
    "\n",
    "import pdb\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "os.chdir(\"/home/jacobheglund/dev/raildelays\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:59:44.037181Z",
     "start_time": "2020-02-27T21:59:44.034860Z"
    }
   },
   "outputs": [],
   "source": [
    "# #TODO 2017 includes XXXPAD and PADXXX trips, remove these or get updated data from Teddy\n",
    "# ## actually, my system of available routes doesn't scale well to include these, so just remove the data lol\n",
    "# path = \"./data/raw/HSP_2017_DID_PAD.txt\"\n",
    "# df_test = pd.read_csv(path)\n",
    "# df_test[\"OD\"] = df_test[\"station_origin\"] + df_test[\"station_destination\"]\n",
    "\n",
    "# np.unique(df_test[\"OD\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:13:25.876959Z",
     "start_time": "2020-02-27T22:13:25.871418Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO append the two data sources (2016, 2017) and use Teddy's maps csv to scale the stop position and \n",
    "# include all these data points\n",
    "\n",
    "raw_data_path = \"./data/raw/HSP_2016_DID_PAD.txt\"\n",
    "adjlist_path = \"./data/processed/raildelays/raildelays_adjlist.txt\"\n",
    "adj_path = \"./data/processed/raildelays/raildelays_adj.npy\"\n",
    "dataset_path = \"./data/processed/raildelays/raildelays.npy\"\n",
    "\n",
    "# plotting options\n",
    "show_plots = False\n",
    "figsize = (16, 9)\n",
    "dpi = 200\n",
    "node_size = 50\n",
    "# xlim, ylim based on min/max lat/long for the included stations in the dataset\n",
    "xlim = (-4.5, 0.5)\n",
    "ylim = (50.75, 52.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link-Based Node Formulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:13:27.813781Z",
     "start_time": "2020-02-27T22:13:27.742485Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(raw_data_path, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:13:28.072583Z",
     "start_time": "2020-02-27T22:13:27.836939Z"
    }
   },
   "outputs": [],
   "source": [
    "# this data was acquired using the station codes to find the station names using the link below,\n",
    "# look them up in Google Maps, and get a lat/long coordinate of the station from Google Maps\n",
    "## https://en.wikipedia.org/wiki/UK_railway_stations_%E2%80%93_A\n",
    "#TODO replace this with ./data/teddy/stations.csv\n",
    "# station map may be found at https://www.gwr.com/plan-journey/stations-and-routes \n",
    "stop_position ={\\\n",
    "\"AML\": ( 51.517207, -0.266751 ),\n",
    "\"APF\": ( 51.639810, -1.242371 ),\n",
    "\"BAN\": ( 52.060522, -1.327883 ),\n",
    "\"BGN\": ( 51.507162, -3.575304 ),\n",
    "\"BNM\": ( 51.522150, -0.645280 ),\n",
    "\"BPW\": ( 51.514626, -2.542716 ),\n",
    "\"BRI\": ( 51.448520, -2.582950 ),\n",
    "\"BTH\": ( 51.377813, -2.356989 ),\n",
    "\"BWT\": ( 51.127593, -2.990609 ),\n",
    "\"CDF\": ( 51.475990, -3.178987 ),\n",
    "\"CMN\": ( 51.853425, -4.305995 ),\n",
    "\"CHO\": ( 51.569495, -1.156961 ),\n",
    "\"CPM\": ( 51.462407, -2.115287 ),\n",
    "\"CUM\": ( 51.653834, -1.236464 ),\n",
    "\"DID\": ( 51.611047, -1.242809 ),\n",
    "\"EAL\": ( 51.514956, -0.301902 ),\n",
    "\"EXD\": ( 50.729359, -3.543525 ),\n",
    "\"FYS\": ( 51.768553, -4.369169 ),\n",
    "\"GOR\": ( 51.521505, -1.132981 ),\n",
    "\"HAY\": ( 51.503028, -0.420389 ),\n",
    "\"HIG\": ( 51.218433, -2.972049 ),\n",
    "\"HYD\": ( 51.918563, -1.299300 ),\n",
    "\"IVR\": ( 51.508453, -0.506122 ),\n",
    "\"KGS\": ( 52.021025, -1.281072 ),\n",
    "\"KWL\": ( 51.734407, -4.317376 ),\n",
    "\"KYN\": ( 51.418035, -2.495436 ),\n",
    "\"LLE\": ( 51.673900, -4.161347 ),\n",
    "\"LNY\": ( 51.508032, -0.541948 ),\n",
    "\"MAI\": ( 51.518727, -0.722769 ),\n",
    "\"NLS\": ( 51.419460, -2.750466 ),\n",
    "\"NTH\": ( 51.662220, -3.806959 ),\n",
    "\"NWP\": ( 51.588709, -3.001131 ),\n",
    "\"OLF\": ( 51.379242, -2.380790 ),\n",
    "\"OXF\": ( 51.753483, -1.269902 ),\n",
    "\"PAD\": ( 51.516683, -0.176897 ),\n",
    "\"PAN\": ( 51.485475, -1.090419 ),\n",
    "\"PBY\": ( 51.684164, -4.248703 ),\n",
    "\"PLY\": ( 50.377667, -4.143794 ),\n",
    "\"PTA\": ( 51.591775, -3.780789 ),\n",
    "\"RAD\": ( 51.686207, -1.240432 ),\n",
    "\"RDG\": ( 51.458404, -0.971437 ),\n",
    "\"SLO\": ( 51.510596, -0.590391 ),\n",
    "\"STL\": ( 51.505362, -0.378839 ),\n",
    "\"SWA\": ( 51.625207, -3.941208 ),\n",
    "\"SWI\": ( 51.565456, -1.785337 ),\n",
    "\"TAC\": ( 51.881350, -1.297397 ),\n",
    "\"TAP\": ( 51.523502, -0.681442 ),\n",
    "\"TAU\": ( 51.023091, -3.102990 ),\n",
    "\"TLH\": ( 51.470890, -1.029031 ),\n",
    "\"TOT\": ( 50.435958, -3.688892 ),\n",
    "\"TVP\": ( 50.917227, -3.359841 ),\n",
    "\"TWY\": ( 51.475204, -0.862948 ),\n",
    "\"WEA\": ( 51.513479, -0.319840 ),\n",
    "\"WDT\": ( 51.510120, -0.472410 ),\n",
    "\"WNM\": ( 51.348586, -2.942140 ),\n",
    "\"WOR\": ( 51.358021, -2.909091 ),\n",
    "\"WSM\": ( 51.344440, -2.972104 ),\n",
    "\"YAT\": ( 51.390918, -2.827768 )}\n",
    "\n",
    "\n",
    "stations = np.unique(df[\"station_curr\"])\n",
    "sps = dict(collections.Counter(df[\"station_curr\"]))\n",
    "sps = dict(sorted(sps.items()))\n",
    "\n",
    "# the corridors are designed with this threshold in mind, it keeps out the \"end of line\" stations \n",
    "# which just add noise to the data\n",
    "# in total this removes 1% of the data \n",
    "\n",
    "#TODO check if the set of filtered stops changes when 2017 is included,\n",
    "## the corridors and available routes WILL change and that's not good\n",
    "## I should update the corridors to fit the new data\n",
    "\n",
    "# stations in 2016 data that are not in 2017 data\n",
    "# AML\n",
    "# PLY\n",
    "# TOT\n",
    "\n",
    "# stations in 2017 data that are not in 2016 data\n",
    "# CBY\n",
    "# DWL\n",
    "# DWW\n",
    "# EXT\n",
    "# NTA\n",
    "# PGN\n",
    "# PWY\n",
    "# SCS\n",
    "# TGM\n",
    "# TQY\n",
    "# TRR\n",
    "\n",
    "stop_thres = 50\n",
    "for i in list(sps):\n",
    "    if sps[i] < stop_thres:\n",
    "        sps.pop(i)\n",
    "        stop_position.pop(i)\n",
    "        df_tmp = df.loc[df[\"station_curr\"] == i]\n",
    "        drop_list = list(df_tmp[\"RID\"])\n",
    "        for j in drop_list:\n",
    "            df = df[df.RID != j]\n",
    "\n",
    "df = df.reset_index()\n",
    "df = df.drop([\"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:13:28.083492Z",
     "start_time": "2020-02-27T22:13:28.073941Z"
    }
   },
   "outputs": [],
   "source": [
    "# - all corridors are defined in linear order\n",
    "# -- once a train gets on a corridor, it travels through all the stations on that corridor\n",
    "# --- although it does not necessarily stop at any of the stations on that corridor\n",
    "\n",
    "# --------------------------------------------\n",
    "# Northwest Corridor\n",
    "# --------------------------------------------\n",
    "# SWA - SWANSEA - START OF CORRIDOR\n",
    "# NTH - NEATH\n",
    "# PTA - PORT TALBOT PARKWAY\n",
    "# BGN - BRIDGEND\n",
    "# CDF - CARDIFF CENTRAL\n",
    "# NWP - NEWPORT\n",
    "# BPW - BRISTOL PARKWAY\n",
    "# SWI - SWINDON - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# Southwest Corridor\n",
    "# --------------------------------------------\n",
    "# TAU - TAUNTON - START OF CORRIDOR\n",
    "# BWT - BRIDGEWATER\n",
    "# HIG - HIGHBRIDGE AND BURNHAM\n",
    "# WSM - WESTON-SUPER-MARE\n",
    "# WNM - WESTON MILTON\n",
    "# WOR - WORLE\n",
    "# YAT - YATTON\n",
    "# NLS - NAILSEA AND BLACKWELL\n",
    "# BRI - BRISTON TEMPLE MEADS\n",
    "# BTH - BATH SPA\n",
    "# CPM - CHIPPENHAM\n",
    "# SWI - SWINDON - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# Middle Connecting Corridor\n",
    "# --------------------------------------------\n",
    "# SWI - SWINDON - START OF CORRIDOR\n",
    "# DID - DIDCOT PARKWAY - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# North Central Corridor\n",
    "# --------------------------------------------\n",
    "# BAN - BANBURY - START OF CORRIDOR\n",
    "# KGS - KINGS SUTTON\n",
    "# HYD - HEYFORD\n",
    "# TAC - TACKLEY\n",
    "# OXF - OXFORD\n",
    "# RAD - RADLEY\n",
    "# CUM - CULHAM\n",
    "# APF - APPLEFORD\n",
    "# DID - DIDCOT PARKWAY - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# Eastern Corridor\n",
    "# --------------------------------------------\n",
    "# DID - DIDCOT PARKWAY\n",
    "# CHO - CHOLSEY\n",
    "# GOR - GORING AND STREATLEY\n",
    "# PAN - PANGBOURNE\n",
    "# TLH - TILEHURST\n",
    "# RDG - READING\n",
    "# TWY - TWYFORD\n",
    "# MAI - MAIDENHEAD\n",
    "# TAP - TAPLOW\n",
    "# BNM - BURNHAM\n",
    "# SLO - SLOUGH\n",
    "# LNY - LANGLEY\n",
    "# IVR - IVER\n",
    "# WDT - WEST DRAYTON\n",
    "# HAY - HAYES AND HARLINGTON\n",
    "# STL - SOUTHALL\n",
    "# EAL - EALING BROADWAY\n",
    "# PAD - LONDON PADDINGTON\n",
    "\n",
    "# used in defining the graph\n",
    "corridor_list = [\n",
    "    [\"SWA\", \"NTH\", \"PTA\", \"BGN\", \"CDF\", \"NWP\", \"BPW\", \"SWI\"],\n",
    "    [\"TAU\", \"BWT\", \"HIG\", \"WSM\", \"WNM\", \"WOR\", \"YAT\", \"NLS\", \"BRI\", \"BTH\", \"CPM\", \"SWI\"],\n",
    "    [\"SWI\", \"DID\"],\n",
    "    [\"BAN\", \"KGS\", \"HYD\", \"TAC\", \"OXF\", \"RAD\", \"CUM\", \"APF\", \"DID\"],\n",
    "    [\"DID\", \"CHO\", \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\", \"TAP\", \"BNM\", \"SLO\", \"LNY\",\n",
    "     \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"]\n",
    "]\n",
    "\n",
    "# rearrange sps with stops grouped by corridor\n",
    "sps_tmp = {}\n",
    "for corridor in corridor_list:\n",
    "    for j in corridor:\n",
    "        keys = list(sps_tmp)\n",
    "        if j not in keys:\n",
    "            sps_tmp[j] = sps[j]\n",
    "sps = sps_tmp\n",
    "\n",
    "# used in finding links between stops\n",
    "available_routes = [\n",
    "    [\"SWA\", \"NTH\", \"PTA\", \"BGN\", \"CDF\", \"NWP\", \"BPW\", \"SWI\", \"DID\", \"CHO\",\n",
    "     \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\", \"TAP\", \"BNM\", \"SLO\", \"LNY\",\n",
    "     \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"],\n",
    "    \n",
    "    [\"TAU\", \"BWT\", \"HIG\", \"WSM\", \"WNM\", \"WOR\", \"YAT\", \"NLS\", \"BRI\", \"BTH\", \n",
    "     \"CPM\", \"SWI\", \"DID\", \"CHO\", \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\",\n",
    "     \"TAP\", \"BNM\", \"SLO\", \"LNY\", \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"],\n",
    "    \n",
    "    [\"BAN\", \"KGS\", \"HYD\", \"TAC\", \"OXF\", \"RAD\", \"CUM\", \"APF\", \"DID\", \"CHO\",\n",
    "     \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\", \"TAP\", \"BNM\", \"SLO\", \"LNY\",\n",
    "     \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:13:28.090200Z",
     "start_time": "2020-02-27T22:13:28.085189Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_links(stop_1, stop_2, available_routes):\n",
    "    # returns a list of all links between two stops on a route\n",
    "    # this relies on having an array of possible routes\n",
    "    #EX: my_list_1 = get_links(\"WDT\", \"PAD\")\n",
    "    # my_list_1 = [\"WDTHHAY\", \"HAYSTL\", \"STLEAL\", \"EALPAD\"]\n",
    "    done = 0\n",
    "    link_list = []\n",
    "    for i in range(len(available_routes)):\n",
    "        curr_route = available_routes[i]\n",
    "        if (stop_1 in curr_route) and (stop_2 in curr_route) and (not done):\n",
    "            done = 1\n",
    "            stop_1_idx = curr_route.index(stop_1)\n",
    "            stop_2_idx = curr_route.index(stop_2)            \n",
    "            for j in range(stop_1_idx, stop_2_idx):                \n",
    "                link_list.append(curr_route[j] + curr_route[j+1])\n",
    "        \n",
    "    return link_list\n",
    "\n",
    "\n",
    "# my_list = get_links(\"NTH\", \"PAD\", available_routes)\n",
    "# my_list = get_links(\"DID\", \"PAD\", available_routes)\n",
    "# my_list = get_links(\"DID\", \"DID\", available_routes)\n",
    "\n",
    "# this situation shouldn't happen since all data ends at PAD\n",
    "# my_list = get_links(\"PAD\", \"DID\", available_routes)\n",
    "\n",
    "# print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:13:28.294853Z",
     "start_time": "2020-02-27T22:13:28.091564Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataframe setup\n",
    "\n",
    "# we're using scheduled times, but there is no reason that we couldn't use actual arrival/departure times\n",
    "## the reason we don't is b/c we would have scheduled tiems available in the real world prediction\n",
    "## maybe a better way would be to use scheduled times in the future predictions,\n",
    "## and actual times in for input data but this just adds needless complecation for a first stab at the problem\n",
    "\n",
    "# OD does not index the links, just the origin and destination of the route\n",
    "df[\"OD\"] = df[\"station_origin\"] + df[\"station_destination\"]\n",
    "\n",
    "# get integer time values for all rows (arrival and departure)\n",
    "df[\"dep_sched_int\"] = df[\"departure_sched\"]\n",
    "df[\"dep_sched_int\"][df[\"dep_sched_int\"] == \"terminating\"] = df[\"arrival_sched\"]\n",
    "df[\"dep_actual_int\"] = df[\"departure_actual\"]\n",
    "df[\"dep_actual_int\"][df[\"dep_actual_int\"] == \"terminating\"] = df[\"arrival_actual\"]\n",
    "\n",
    "# this one is different b/c \"starting\" arrival delay must be 0\n",
    "df[\"arr_sched_int\"] = df[\"arrival_sched\"]\n",
    "df[\"arr_sched_int\"][df[\"arr_sched_int\"] == \"starting\"] = df[\"departure_actual\"]\n",
    "df[\"arr_actual_int\"] = df[\"arrival_actual\"]\n",
    "df[\"arr_actual_int\"][df[\"arr_actual_int\"] == \"starting\"] = df[\"departure_actual\"]\n",
    "\n",
    "\n",
    "# get scheduled arrival and departure as datetime\n",
    "df[\"arr_sched_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"arr_sched_int\"].astype(str))\n",
    "df[\"dep_sched_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"dep_sched_int\"].astype(str))\n",
    "df[\"arr_actual_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"arr_actual_int\"].astype(str))\n",
    "df[\"dep_actual_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"dep_actual_int\"].astype(str)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:04.204769Z",
     "start_time": "2020-02-27T22:13:28.296255Z"
    }
   },
   "outputs": [],
   "source": [
    "# find the delay that was generated by the the previous section of track for each stop\n",
    "## arrival delay is positive if the train was late, and negative if the train was early\n",
    "# get the set of links each train travelled over between consecutive stops\n",
    "\n",
    "df[\"cum_arr_delay\"] = ((df[\"arr_actual_datetime\"] - df[\"arr_sched_datetime\"]).dt.total_seconds()) / 60\n",
    "init_list = []\n",
    "for i in range(len(df)):\n",
    "    init_list.append([])\n",
    "df[\"prev_links\"] = init_list\n",
    "\n",
    "# make sure we're only combining data from one train trip at a time\n",
    "for row_count in range(len(df)):\n",
    "    curr_row = df[row_count:row_count+1]\n",
    "    if (curr_row[\"arrival_sched\"].item() == \"starting\"):\n",
    "        df.at[row_count, \"prev_link_datetime\"] = df.at[row_count, \"dep_sched_datetime\"]\n",
    "        df.at[row_count, \"prev_link_delay\"] = 0.\n",
    "        df.at[row_count, \"prev_links\"] = []\n",
    "\n",
    "    else:\n",
    "        prev_row = df[row_count-1:row_count]\n",
    "\n",
    "        t0 = prev_row[\"dep_sched_datetime\"].item()\n",
    "        t1 = curr_row[\"arr_sched_datetime\"].item()\n",
    "\n",
    "        # average time at which the train was running on previous link\n",
    "        df.at[row_count, \"prev_link_datetime\"] = t0 + ((t1-t0) / 2)\n",
    "        # df.at[row_count, \"prev_link_datetime\"] = pd.Timestamp(2002, 5, 5)\n",
    "        df.at[row_count, \"prev_link_delay\"] = curr_row[\"cum_arr_delay\"].item() - prev_row[\"cum_arr_delay\"].item()\n",
    "        df.at[row_count, \"prev_links\"] = get_links(prev_row[\"station_curr\"].item(), curr_row[\"station_curr\"].item(), available_routes)\n",
    "\n",
    "\n",
    "    row_count += 1\n",
    "        \n",
    "# alphabetize columns\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation and Visualization of Rail Network Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:04.211429Z",
     "start_time": "2020-02-27T22:14:04.206943Z"
    }
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "stop_idx = {}\n",
    "idx_stop = {}\n",
    "for i in list(sps):\n",
    "    stop_idx[i] = counter\n",
    "    idx_stop[counter] = str(i)\n",
    "    counter += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:04.218177Z",
     "start_time": "2020-02-27T22:14:04.213670Z"
    }
   },
   "outputs": [],
   "source": [
    "# # processing to verify the corridors I defined\n",
    "# # Southwest Corridor\n",
    "# # check if any trains starting in TAU go through BPW or if they all go through SWI\n",
    "# ## Result: They all go through SWI, so the SW Corridor goes from TAU to SWI without branching off in this dataset\n",
    "# ## Result: The Northwest and Southwest Corridors only meet at SWI for this dataset\n",
    "\n",
    "# # North Central Corridor\n",
    "# ## all trains going from BAN to PAD must pass through DID to be part of this dataset\n",
    "# ## therefore there are no trains going from APF to CHO\n",
    "# ## no trains go from BAN and pass through SWI\n",
    "\n",
    "# df1 = df.copy()\n",
    "# df1 = df1.loc[df1[\"station_origin\"] == \"BAN\"]\n",
    "# df1 = df1.loc[df1[\"station_curr\"] == \"SWI\"]\n",
    "\n",
    "\n",
    "# df1.head()\n",
    "# print(len(df1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:04.228747Z",
     "start_time": "2020-02-27T22:14:04.219979Z"
    }
   },
   "outputs": [],
   "source": [
    "# create graph representing rail network\n",
    "directed_graph = False\n",
    "#TODO add option for directed adjacency s.t. it follows the direction of traffic\n",
    "## for this particular dataset where everything starts at XXX and ends at PAD\n",
    "## i would need to see how other traffic papers accomplish this\n",
    "\n",
    "total_str = \"\"\n",
    "for corridor in corridor_list:\n",
    "    if directed_graph:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        for j in range(len(corridor)):\n",
    "            tmp_str = \"\"\n",
    "            if (j == 0):\n",
    "                tmp_str += str(stop_idx[corridor[j]])\n",
    "                tmp_str += \" \"\n",
    "                tmp_str += str(stop_idx[corridor[j+1]])\n",
    "\n",
    "            elif (j == len(corridor)-1):\n",
    "                tmp_str += str(stop_idx[corridor[j]])\n",
    "                tmp_str += \" \"\n",
    "                tmp_str += str(stop_idx[corridor[j-1]])\n",
    "\n",
    "            else:\n",
    "                tmp_str += str(stop_idx[corridor[j]])\n",
    "                tmp_str += \" \"\n",
    "                tmp_str += str(stop_idx[corridor[j-1]])\n",
    "                tmp_str += \" \"\n",
    "                tmp_str += str(stop_idx[corridor[j+1]])\n",
    "\n",
    "            total_str += tmp_str\n",
    "            total_str += \"\\n\"\n",
    "        \n",
    "with open(adjlist_path, \"w\") as f:\n",
    "    f.write(total_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:04.232918Z",
     "start_time": "2020-02-27T22:14:04.229959Z"
    }
   },
   "outputs": [],
   "source": [
    "idx_position = {}\n",
    "counter = 0\n",
    "for i in stop_position:\n",
    "    idx_position[stop_idx[i]] = (stop_position[i][1],stop_position[i][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:04.238658Z",
     "start_time": "2020-02-27T22:14:04.233877Z"
    }
   },
   "outputs": [],
   "source": [
    "G = nx.read_adjlist(adjlist_path, nodetype=int)\n",
    "if show_plots:\n",
    "    fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    nx.draw(G, idx_position, node_size=node_size)\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:04.245307Z",
     "start_time": "2020-02-27T22:14:04.240975Z"
    }
   },
   "outputs": [],
   "source": [
    "if show_plots:\n",
    "    fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    nx.draw_networkx_labels(G, idx_position, idx_stop, font_size=10)\n",
    "    nx.draw_networkx_edges(G, idx_position)\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:04.250313Z",
     "start_time": "2020-02-27T22:14:04.247022Z"
    }
   },
   "outputs": [],
   "source": [
    "LG = nx.line_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:04.258050Z",
     "start_time": "2020-02-27T22:14:04.252101Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the position of the edges as average of station positions\n",
    "edge_position = {}\n",
    "for edge in LG.nodes():\n",
    "    station_1 = int(edge[0])\n",
    "    station_2 = int(edge[1])\n",
    "    x1, x2 = idx_position[station_1][0], idx_position[station_2][0]\n",
    "    y1, y2 = idx_position[station_1][1], idx_position[station_2][1]\n",
    "    \n",
    "    x_edge = (x1+x2)/2\n",
    "    y_edge = (y1+y2)/2\n",
    "    edge_position[edge] = (x_edge, y_edge) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:04.269817Z",
     "start_time": "2020-02-27T22:14:04.261000Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get edge labels\n",
    "edge_label = {}\n",
    "for edge in LG.nodes():\n",
    "    station_1 = int(edge[0])\n",
    "    station_2 = int(edge[1])\n",
    "    \n",
    "    # TODO i'm not sure why these two cases mess it up, but\n",
    "    # SWICPM should be CPMSWI\n",
    "    # DIDAPF should be APFDID\n",
    "    # something about the fact that they have multiple connections or are part of a complex junction\n",
    "\n",
    "    if (idx_stop[station_1] == \"SWI\") and (idx_stop[station_2] == \"CPM\") or \\\n",
    "        (idx_stop[station_1] == \"DID\") and (idx_stop[station_2] == \"APF\"):\n",
    "        station_tmp = station_1\n",
    "        station_1 = station_2\n",
    "        station_2 = station_tmp\n",
    "    \n",
    "    edge_label[edge] = idx_stop[station_1] + idx_stop[station_2]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:04.275655Z",
     "start_time": "2020-02-27T22:14:04.271526Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if show_plots:\n",
    "    fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    nx.draw(LG, edge_position, node_size=node_size)\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:04.281810Z",
     "start_time": "2020-02-27T22:14:04.277462Z"
    }
   },
   "outputs": [],
   "source": [
    "if show_plots:\n",
    "    fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    nx.draw_networkx_labels(LG, edge_position, edge_label, font_size=7)\n",
    "    nx.draw_networkx_edges(LG, edge_position)\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:04.289234Z",
     "start_time": "2020-02-27T22:14:04.283277Z"
    }
   },
   "outputs": [],
   "source": [
    "# add self loops to graph\n",
    "for i in LG.nodes:\n",
    "    LG.add_edge(i, i)\n",
    "\n",
    "adj = nx.adjacency_matrix(LG).todense()\n",
    "adj = np.array(adj).astype(\"d\")\n",
    "np.save(adj_path, adj)\n",
    "if show_plots:\n",
    "    plt.matshow(adj)\n",
    "    plt.show()\n",
    "# TODO maybe change indexing of stations so that it's not alphabetical, but done with stations \"close\" in space\n",
    "# (or on the same corridors) as having consecutive indicies\n",
    "## all this will do is make the adj look nicer, which is probably desirable for the paper just so people \n",
    "## don't get confused\n",
    "## this didn't work, so whatever, I'll come back to it if it's important enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Rail Link Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:04.360281Z",
     "start_time": "2020-02-27T22:14:04.290367Z"
    }
   },
   "outputs": [],
   "source": [
    "# find starting and ending time\n",
    "dt = 30 # minutes\n",
    "\n",
    "def round_time(t_input, dt, direction=\"floor\"):\n",
    "    # \n",
    "    if direction == \"floor\":\n",
    "        n_minutes = ((pd.to_timedelta(str(t_input[0:2] + \":\" + t_input[2:4] + \":00\")).seconds / 60) // dt * dt)\n",
    "        return pd.to_timedelta(n_minutes, unit=\"m\")\n",
    "    \n",
    "    elif direction == \"ceil\":\n",
    "        n_minutes = ((pd.to_timedelta(str(t_input[0:2] + \":\" + t_input[2:4] + \":00\")).seconds / 60 + dt) // dt * dt)\n",
    "        return pd.to_timedelta(n_minutes, unit=\"m\")\n",
    "    \n",
    "t_min = min(df[\"dep_sched_int\"])\n",
    "t_max = max(df[\"dep_sched_int\"])\n",
    "t_start = round_time(t_min, dt)\n",
    "t_end = round_time(t_max, dt, \"ceil\")\n",
    "\n",
    "# unique times during each day\n",
    "time_list = []\n",
    "t_curr = t_start\n",
    "\n",
    "while t_curr < t_end:\n",
    "    time_list.append(t_curr)\n",
    "    t_curr += pd.Timedelta(str(dt) + \"minutes\")\n",
    "\n",
    "# unique dates of the year\n",
    "date_list = np.unique(df[\"date\"])\n",
    "\n",
    "datetime_list = []\n",
    "for i in date_list:\n",
    "    t0 = pd.to_datetime(i + \" 00:00:00\")\n",
    "        \n",
    "    for j in time_list:\n",
    "        t_curr = t0 + j\n",
    "        datetime_list.append(t_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:04.371251Z",
     "start_time": "2020-02-27T22:14:04.363055Z"
    }
   },
   "outputs": [],
   "source": [
    "## construct useful mappings\n",
    "\n",
    "# mapping from node description to unique index\n",
    "LG_node_idx = {}\n",
    "# mapping from unique index to node description \n",
    "LG_idx_node = {}\n",
    "counter = 0\n",
    "for i in LG.nodes():\n",
    "    LG_node_idx[i] = counter\n",
    "    LG_idx_node[counter] = i\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "# mapping from node description to label (i.e. \"SWAPAD\")\n",
    "LG_node_label = edge_label\n",
    "# mapping from label to node description\n",
    "LG_label_node = {}\n",
    "for i in LG_node_label:\n",
    "    LG_label_node[LG_node_label[i]] = i\n",
    "\n",
    "\n",
    "# mapping from time of year to time data index\n",
    "datetime_idx = {}\n",
    "# mapping from time data index to time of year\n",
    "idx_datetime = {}\n",
    "counter = 0\n",
    "for i in datetime_list:\n",
    "    datetime_idx[i] = counter\n",
    "    idx_datetime[counter] = i\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:14:23.458256Z",
     "start_time": "2020-02-27T22:14:04.372782Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_timesteps_per_day: 15.0\n"
     ]
    }
   ],
   "source": [
    "n_nodes = len(adj)\n",
    "n_timesteps = len(datetime_list)\n",
    "n_features = 1 #  delay per link\n",
    "n_timesteps_per_day = ((t_end - t_start)/dt).seconds / 60\n",
    "print(\"n_timesteps_per_day:\", n_timesteps_per_day)\n",
    "delay_dataset = np.zeros((n_timesteps, n_nodes, n_features))\n",
    "n_trains_total = np.zeros((n_nodes, 1))\n",
    "\n",
    "for i in range(len(datetime_list)-1):\n",
    "    # get set of trains running during current time period\n",
    "    t0, t1 = datetime_list[i], datetime_list[i+1]\n",
    "    df_tmp = df.loc[(t0 <= df[\"prev_link_datetime\"]) & (df[\"prev_link_datetime\"] <= t1)]\n",
    "    n_trains_link = np.zeros((n_nodes, 1))\n",
    "\n",
    "    for j in range(len(df_tmp)):\n",
    "        curr_row = df_tmp[j:j+1]\n",
    "\n",
    "        # get list of affected links\n",
    "        prev_links = curr_row[\"prev_links\"].item()\n",
    "        n_trains = len(prev_links)\n",
    "\n",
    "        for k in range(len(prev_links)):\n",
    "            link = prev_links[k]\n",
    "            link_idx = LG_node_idx[LG_label_node[link]]\n",
    "\n",
    "            # sum the total delay experienced by trains along this link during the time period\n",
    "            # those links into delay_dataset\n",
    "            delay_dataset[i, link_idx, 0] += curr_row[\"prev_link_delay\"].item()\n",
    "            \n",
    "            # enumerate trains that ran over each link during the time period\n",
    "            n_trains_link[link_idx] += 1\n",
    "            \n",
    "    n_trains_total += n_trains_link\n",
    "    # get the delay over each link averaged by number of trains that passed through the link\n",
    "    divisor = n_trains_link + np.where(n_trains_link == 0, 1, 0)\n",
    "    \n",
    "    delay_dataset[i, :, :] = delay_dataset[i, :, :] / divisor\n",
    "\n",
    "# # average number of trains per day per link\n",
    "# print(n_trains_total / len(date_list))\n",
    "np.save(dataset_path, delay_dataset)\n",
    "\n",
    "# delay_dataset.shape = (n_timesteps_during_data_period, n_nodes, n_input_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
